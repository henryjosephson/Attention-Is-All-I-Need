# Attention Is All I Need
This is my attempt to replicate the classic 2017 paper [*Attention Is All You Need*](https://arxiv.org/abs/1706.03762), which introduced the transformer architecture.

Why? Because it's fun, because I want to understand how transformers actually work, and because 

I'm far from the first person to do this â€” see previous replications by [Martin Dittgen](https://medium.com/@martin.p.dittgen/reproducing-the-attention-is-all-you-need-paper-from-scratch-d2fb40bb25d4), 